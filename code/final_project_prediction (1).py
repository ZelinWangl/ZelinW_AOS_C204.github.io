# -*- coding: utf-8 -*-
"""Final_Project_Prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jpZoqRudpf4rYkAGJxZyy96iQ_RTExBu
"""

import os
from matplotlib import pyplot as plt
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LinearRegression
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.neural_network import MLPRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import warnings
warnings.filterwarnings('ignore')

from google.colab import drive
drive.mount('/content/drive')
filepath = '/content/drive/MyDrive/C204_Notebooks/Bias_correction_ucl.csv'

file_base = pd.read_csv(filepath)
#display(file_base)
file_base.describe()
#print(file_base.shape)

print(file_base['Date'])

# to visualize the correlations between Max Temp and each input feature independently (i.e. plot Present_Tmax vs. Max temperature Next day and compute the correlation coefficient)
features = ['Present_Tmax', 'Present_Tmin', 'LDAPS_RHmin', 'LDAPS_RHmax',
            'LDAPS_Tmax_lapse', 'LDAPS_Tmin_lapse', 'LDAPS_WS', 'LDAPS_LH',
            'LDAPS_CC1', 'LDAPS_CC2', 'LDAPS_CC3', 'LDAPS_CC4',
            'LDAPS_PPT1', 'LDAPS_PPT2', 'LDAPS_PPT3', 'LDAPS_PPT4',
            'DEM', 'Slope', 'Solar radiation']

feature_cols = features + ['Next_Tmax']
file_clean = file_base[feature_cols].dropna()  # remove NAN rows

# calculate the correlations between Tmax_nextday and any feature
correlations = []
for feature in features:
    corr, p_value = stats.pearsonr(file_clean[feature], file_clean['Next_Tmax'])
    correlations.append({'Feature': feature, 'Correlation': corr, 'P-value': p_value})

corr_df = pd.DataFrame(correlations).sort_values('Correlation', key=abs, ascending=False)

# Plot: Correlation Coefficient
fig, ax = plt.subplots(figsize=(12, 8))
colors = ['red' if c < 0 else 'green' for c in corr_df['Correlation']]

y_positions = np.arange(len(corr_df))
correlations_vals = corr_df['Correlation'].values
feature_names = corr_df['Feature'].values

bars = ax.barh(y_positions, correlations_vals, color=colors, alpha=0.7, height=0.7)
ax.axvline(x=0, color='black', linewidth=1.5)
ax.set_xlabel('Pearson Correlation Coefficient', fontsize=12, fontweight='bold')
ax.set_ylabel('Features', fontsize=12, fontweight='bold')
ax.set_title('Correlation between Features and Next_day Max Temperature', fontsize=14, fontweight='bold')
ax.set_yticks(y_positions)
ax.set_yticklabels(feature_names, fontsize=10)
ax.grid(True, alpha=0.3, axis='x')

for i, (pos, corr) in enumerate(zip(y_positions, correlations_vals)):
    ax.text(corr + 0.01 if corr > 0 else corr - 0.01, pos, f'{corr:.3f}',
            va='center', ha='left' if corr > 0 else 'right', fontsize=8)

plt.tight_layout()
plt.savefig('correlation_barplot.png', dpi=300, bbox_inches='tight')
plt.show()

top_features = corr_df.head(6)['Feature'].tolist()
fig, axes = plt.subplots(2, 3, figsize=(18, 12))
axes = axes.flatten()

for idx, feature in enumerate(top_features):
    x_data = file_clean[feature].values
    y_data = file_clean['Next_Tmax'].values

    axes[idx].scatter(x_data, y_data, alpha=0.3, s=10, color='steelblue')

    if len(x_data) > 1:
        z = np.polyfit(x_data, y_data, 1)
        p = np.poly1d(z)
        x_line = np.linspace(x_data.min(), x_data.max(), 100)
        axes[idx].plot(x_line, p(x_line), "r-", linewidth=2, label='Trend')

    corr = corr_df[corr_df['Feature'] == feature]['Correlation'].values[0]
    axes[idx].text(0.05, 0.95, f'r = {corr:.3f}',
                  transform=axes[idx].transAxes, fontsize=11,
                  verticalalignment='top',
                  bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))

    axes[idx].axhline(y=0, color='gray', linestyle='--', linewidth=1)
    axes[idx].set_xlabel(feature, fontsize=11, fontweight='bold')
    axes[idx].set_ylabel('Next_day Max Temperature (°C)', fontsize=11, fontweight='bold')
    axes[idx].grid(True, alpha=0.3)
    axes[idx].legend()

plt.suptitle('Scatter Plots: Top 6 Correlated Features vs Next_day Max Temperature',
             fontsize=15, fontweight='bold')
plt.tight_layout()
plt.savefig('scatter_top_features.png', dpi=300, bbox_inches='tight')
plt.show()

"""Data Pre-processing

"""

# Set up the features, and the target
# using all features to predict
features_exp1 = [
    'Present_Tmax', 'Present_Tmin',
    'LDAPS_RHmin', 'LDAPS_RHmax',
    'LDAPS_Tmax_lapse', 'LDAPS_Tmin_lapse',
    'LDAPS_WS', 'LDAPS_LH',
    'LDAPS_CC1', 'LDAPS_CC2', 'LDAPS_CC3', 'LDAPS_CC4',
    'LDAPS_PPT1', 'LDAPS_PPT2', 'LDAPS_PPT3', 'LDAPS_PPT4',
    'lat', 'lon', 'DEM', 'Slope', 'Solar radiation'
]

# exclude the LDAPS Tmax Prediction
features_exp2 = [
    'Present_Tmax', 'Present_Tmin', 'LDAPS_RHmin', 'LDAPS_RHmax',
    'LDAPS_WS', 'LDAPS_LH',
    'LDAPS_CC1', 'LDAPS_CC2', 'LDAPS_CC3', 'LDAPS_CC4',
    'LDAPS_PPT1', 'LDAPS_PPT2', 'LDAPS_PPT3', 'LDAPS_PPT4',
    'lat', 'lon', 'DEM', 'Slope', 'Solar radiation'
]

target = ['Next_Tmax']

"""Spliting the Data"""

#
df = file_base.dropna()
print(df.shape)

y = target

# Spliting the Data

split_idx = int(len(df) * 0.7)
train_df = df.iloc[:split_idx]
test_df = df.iloc[split_idx:]

print(f"Training Size: {len(train_df)} ({len(train_df)/len(df)*100:.1f}%)")
print(f"Testing Size: {len(test_df)} ({len(test_df)/len(df)*100:.1f}%)")

# A set of models
models = {
    'Linear Regression': LinearRegression(),
    'Decision Tree': DecisionTreeRegressor(max_depth=10, min_samples_split=20, random_state=42),
    'Random Forest': RandomForestRegressor(n_estimators=100, max_depth=15, min_samples_split=10, random_state=42),
    'MLP Regressor': MLPRegressor(hidden_layer_sizes=(100, 50), max_iter=500, random_state=42, early_stopping=True)
}

# Store the Result here
results = {
    'Experiment 1 (with LDAPS-nextdayTmax)': {},
    'Experiment 2 (without LDAPS-nextdayTmax)': {}
}

feature_importance = {
    'Experiment 1': {},
    'Experiment 2': {}
}

##################################################3
# experiment 1: LDAPS-nextday Tmax included

X1_train = train_df[features_exp1]
X1_test = test_df[features_exp1]
y1_train = train_df[target]
y1_test = test_df[target]

# Normalization
scaler1 = StandardScaler()
X1_train_scaled = scaler1.fit_transform(X1_train)
X1_test_scaled = scaler1.transform(X1_test)

for model_name, model in models.items():

    # scaled data applied for Linaer Regression and MLP Regressor
    if model_name in ['Linear Regression', 'MLP Regressor']:
        model.fit(X1_train_scaled, y1_train)
        y_pred = model.predict(X1_test_scaled)
    else:
        model.fit(X1_train, y1_train)
        y_pred = model.predict(X1_test)

    # Evaluation
    mae = mean_absolute_error(y1_test, y_pred)
    rmse = np.sqrt(mean_squared_error(y1_test, y_pred))
    r2 = r2_score(y1_test, y_pred)

    results['Experiment 1 (with LDAPS-nextdayTmax)'][model_name] = {
        'MAE': mae,
        'RMSE': rmse,
        'R2': r2
    }
    print(model_name)
    print(f"  MAE:  {mae:.4f}°C")
    print(f"  RMSE: {rmse:.4f}°C")
    print(f"  R²:   {r2:.4f}")

    # Importance of the features (For the Decision Tree/ Random Forest)
    if model_name in ['Decision Tree', 'Random Forest']:
        importances = model.feature_importances_
        feature_importance['Experiment 1'][model_name] = pd.DataFrame({
            'Feature': features_exp1,
            'Importance': importances
        }).sort_values('Importance', ascending=False)

# Prediction vs Actual
lr_pred = models['Linear Regression'].predict(X1_test_scaled)
dt_pred = models['Decision Tree'].predict(X1_test)
rf_pred = models['Random Forest'].predict(X1_test)
mlp_pred = models['MLP Regressor'].predict(X1_test_scaled)

predictions = [lr_pred, dt_pred, rf_pred, mlp_pred]
titles = ['Linear Regression', 'Decision Tree', 'Random Forest', 'MLP Regressor']

fig, axes = plt.subplots(2, 2, figsize=(14, 12))
axes = axes.flatten()

for i in range(4):
    axes[i].scatter(y1_test, predictions[i], alpha=0.5, s=10)
    axes[i].plot([y1_test.min(), y1_test.max()], [y1_test.min(), y1_test.max()],
                 'r--', lw=2)
    axes[i].set_xlabel('Actual Temperature (°C)')
    axes[i].set_ylabel('Predicted Temperature (°C)')
    axes[i].set_title(titles[i])
    axes[i].grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig('prediction_vs_actual.png', dpi=150)
plt.show()

###################################################
# Decision Tree Structure

from sklearn.tree import plot_tree

plt.figure(figsize=(20, 10))
plot_tree(models['Decision Tree'],
          feature_names=features_exp1,
          filled=True,
          rounded=True,
          max_depth=3,
          fontsize=8)
plt.title('Decision Tree Structure')
plt.savefig('dt_structure.png', dpi=150)
plt.show()

###################################################

# Random Forest - First Tree Structure
plt.figure(figsize=(20, 10))
plot_tree(models['Random Forest'].estimators_[0],
          feature_names=features_exp1,
          filled=True,
          rounded=True,
          max_depth=3,
          fontsize=8)
plt.title('Random Forest - First Tree')
plt.savefig('rf_tree_structure.png', dpi=150)
plt.show()

# Feature Importance - Decision Tree
dt_importance = feature_importance['Experiment 1']['Decision Tree'].head(10)

plt.figure(figsize=(10, 6))
plt.barh(dt_importance['Feature'], dt_importance['Importance'])
plt.xlabel('Importance')
plt.ylabel('Feature')
plt.title('Decision Tree - Top 10 Important Features')
plt.gca().invert_yaxis()
plt.savefig('dt_feature_importance.png', dpi=150)
plt.show()

# Feature Importance - Random Forest
rf_importance = feature_importance['Experiment 1']['Random Forest'].head(10)

plt.figure(figsize=(10, 6))
plt.barh(rf_importance['Feature'], rf_importance['Importance'])
plt.xlabel('Importance')
plt.ylabel('Feature')
plt.title('Random Forest - Top 10 Important Features')
plt.gca().invert_yaxis()
plt.savefig('rf_feature_importance.png', dpi=150)
plt.show()

##################################################3
# experiment 2: LDAPS-nextday Tmax excluded

X2_train = train_df[features_exp2]
X2_test = test_df[features_exp2]
y2_train = train_df[target]
y2_test = test_df[target]

# Normalization
scaler2 = StandardScaler()
X2_train_scaled = scaler2.fit_transform(X2_train)
X2_test_scaled = scaler2.transform(X2_test)

for model_name, model in models.items():

    # scaled data applied for Linaer Regression and MLP Regressor
    if model_name in ['Linear Regression', 'MLP Regressor']:
        model.fit(X2_train_scaled, y2_train)
        y_pred = model.predict(X2_test_scaled)
    else:
        model.fit(X2_train, y2_train)
        y_pred = model.predict(X2_test)

    # Evaluation
    mae = mean_absolute_error(y2_test, y_pred)
    rmse = np.sqrt(mean_squared_error(y2_test, y_pred))
    r2 = r2_score(y2_test, y_pred)

    results['Experiment 2 (without LDAPS-nextdayTmax)'][model_name] = {
        'MAE': mae,
        'RMSE': rmse,
        'R2': r2
    }
    print(model_name)
    print(f"  MAE:  {mae:.4f}°C")
    print(f"  RMSE: {rmse:.4f}°C")
    print(f"  R²:   {r2:.4f}")

    # Importance of the features (For the Decision Tree/ Random Forest)
    if model_name in ['Decision Tree', 'Random Forest']:
        importances = model.feature_importances_
        feature_importance['Experiment 2'][model_name] = pd.DataFrame({
            'Feature': features_exp2,
            'Importance': importances
        }).sort_values('Importance', ascending=False)

# Prediction vs Actual
lr_pred = models['Linear Regression'].predict(X2_test_scaled)
dt_pred = models['Decision Tree'].predict(X2_test)
rf_pred = models['Random Forest'].predict(X2_test)
mlp_pred = models['MLP Regressor'].predict(X2_test_scaled)

predictions = [lr_pred, dt_pred, rf_pred, mlp_pred]
titles = ['Linear Regression', 'Decision Tree', 'Random Forest', 'MLP Regressor']

fig, axes = plt.subplots(2, 2, figsize=(14, 12))
axes = axes.flatten()

for i in range(4):
    axes[i].scatter(y2_test, predictions[i], alpha=0.5, s=10)
    axes[i].plot([y2_test.min(), y2_test.max()], [y2_test.min(), y2_test.max()],
                 'r--', lw=2)
    axes[i].set_xlabel('Actual Temperature (°C)')
    axes[i].set_ylabel('Predicted Temperature (°C)')
    axes[i].set_title(titles[i])
    axes[i].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

###################################################
# Decision Tree Structure

from sklearn.tree import plot_tree

plt.figure(figsize=(20, 10))
plot_tree(models['Decision Tree'],
          feature_names=features_exp2,
          filled=True,
          rounded=True,
          max_depth=3,
          fontsize=8)
plt.title('Decision Tree Structure')
plt.show()

###################################################

# Random Forest - First Tree Structure
plt.figure(figsize=(20, 10))
plot_tree(models['Random Forest'].estimators_[0],
          feature_names=features_exp2,
          filled=True,
          rounded=True,
          max_depth=3,
          fontsize=8)
plt.title('Random Forest - First Tree')
plt.show()

# Feature Importance - Decision Tree
dt_importance = feature_importance['Experiment 2']['Decision Tree'].head(10)

plt.figure(figsize=(10, 6))
plt.barh(dt_importance['Feature'], dt_importance['Importance'])
plt.xlabel('Importance')
plt.ylabel('Feature')
plt.title('Decision Tree - Top 10 Important Features')
plt.gca().invert_yaxis()
plt.show()

# Feature Importance - Random Forest
rf_importance = feature_importance['Experiment 2']['Random Forest'].head(10)

plt.figure(figsize=(10, 6))
plt.barh(rf_importance['Feature'], rf_importance['Importance'])
plt.xlabel('Importance')
plt.ylabel('Feature')
plt.title('Random Forest - Top 10 Important Features')
plt.gca().invert_yaxis()
plt.show()

ldaps_pred = test_df['LDAPS_Tmax_lapse']
ldaps_actual = test_df['Next_Tmax']

ldaps_mae = mean_absolute_error(ldaps_actual, ldaps_pred)
ldaps_rmse = np.sqrt(mean_squared_error(ldaps_actual, ldaps_pred))
ldaps_r2 = r2_score(ldaps_actual, ldaps_pred)

print('LDAPS')
print(f"MAE:  {ldaps_mae:.4f}°C")
print(f"RMSE: {ldaps_rmse:.4f}°C")
print(f"R²:   {ldaps_r2:.4f}")

model_names = ['Linear Regression', 'Decision Tree', 'Random Forest', 'MLP Regressor']
exp1_rmse = [results['Experiment 1 (with LDAPS-nextdayTmax)'][m]['RMSE'] for m in model_names]
exp2_rmse = [results['Experiment 2 (without LDAPS-nextdayTmax)'][m]['RMSE'] for m in model_names]

x = np.arange(len(model_names))
width = 0.35

fig, ax = plt.subplots(figsize=(12, 6))
bars1 = ax.bar(x - width/2, exp1_rmse, width, label='Exp1 (with LDAPS_next-day Tmax)',
               color='steelblue', alpha=0.8)
bars2 = ax.bar(x + width/2, exp2_rmse, width, label='Exp2 (without LDAPS_next-day Tmax)',
               color='coral', alpha=0.8)

# LDAPS-predicted rmse
ax.axhline(y=ldaps_rmse, color='red', linestyle='--', linewidth=2.5,
           label=f'LDAPS Original (RMSE={ldaps_rmse:.3f}°C)')

ax.set_xlabel('Models', fontsize=12, fontweight='bold')
ax.set_ylabel('RMSE (°C)', fontsize=12, fontweight='bold')
ax.set_title('RMSE Comparison: Exp1 vs Exp2 vs LDAPS Original Prediction',
             fontsize=14, fontweight='bold')
ax.set_xticks(x)
ax.set_xticklabels(model_names, rotation=15, ha='right')
ax.legend(fontsize=10)
ax.grid(True, alpha=0.3, axis='y')

for bars in [bars1, bars2]:
    for bar in bars:
        height = bar.get_height()
        ax.text(bar.get_x() + bar.get_width()/2., height,
                f'{height:.3f}',
                ha='center', va='bottom', fontsize=8)

plt.tight_layout()
plt.savefig('rmse_comparison_with_ldaps.png', dpi=150)
plt.show()

print("\nExperiment 1 (with LDAPS-nextdayTmax):")
for m, rmse in zip(model_names, exp1_rmse):
    diff = ldaps_rmse - rmse
    print(f"  {m:20s}: {rmse:.4f}°C (Improvement: {diff:+.4f}°C)")

print("\nExperiment 2 (without LDAPS-nextdayTmax):")
for m, rmse in zip(model_names, exp2_rmse):
    diff = ldaps_rmse - rmse
    print(f"  {m:20s}: {rmse:.4f}°C (Improvement: {diff:+.4f}°C)")